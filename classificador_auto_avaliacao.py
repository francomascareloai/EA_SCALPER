#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ CLASSIFICADOR COM AUTO-AVALIA√á√ÉO CONT√çNUA
Sistema inteligente que se auto-monitora e melhora continuamente

Recursos:
- Auto-avalia√ß√£o a cada N arquivos
- M√©tricas de qualidade em tempo real
- Ajustes autom√°ticos de par√¢metros
- Relat√≥rios de melhoria cont√≠nua
- Detec√ß√£o de padr√µes emergentes
"""

import os
import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, asdict
import statistics

@dataclass
class MetricasAutoAvaliacao:
    """M√©tricas para auto-avalia√ß√£o do processo"""
    total_arquivos: int = 0
    deteccoes_corretas: int = 0
    deteccoes_incertas: int = 0
    casos_especiais_detectados: int = 0
    nomenclatura_consistente: int = 0
    ftmo_compliance_detectado: int = 0
    confidence_scores: List[float] = None
    tempo_processamento: List[float] = None
    
    def __post_init__(self):
        if self.confidence_scores is None:
            self.confidence_scores = []
        if self.tempo_processamento is None:
            self.tempo_processamento = []
    
    @property
    def taxa_precisao(self) -> float:
        """Taxa de precis√£o das detec√ß√µes"""
        if self.total_arquivos == 0:
            return 0.0
        return (self.deteccoes_corretas / self.total_arquivos) * 100
    
    @property
    def confidence_media(self) -> float:
        """Confidence score m√©dio"""
        return statistics.mean(self.confidence_scores) if self.confidence_scores else 0.0
    
    @property
    def tempo_medio(self) -> float:
        """Tempo m√©dio de processamento por arquivo"""
        return statistics.mean(self.tempo_processamento) if self.tempo_processamento else 0.0

class AutoAvaliadorQualidade:
    """Sistema de auto-avalia√ß√£o cont√≠nua"""
    
    def __init__(self, intervalo_avaliacao: int = 10):
        self.intervalo_avaliacao = intervalo_avaliacao
        self.metricas = MetricasAutoAvaliacao()
        self.historico_avaliacoes = []
        self.padroes_emergentes = {}
        self.ajustes_realizados = []
        
        # Thresholds para qualidade
        self.thresholds = {
            'confidence_minimo': 70.0,
            'taxa_precisao_minima': 85.0,
            'tempo_maximo_por_arquivo': 5.0,
            'casos_especiais_minimos': 5.0
        }
    
    def avaliar_deteccao_tipo(self, codigo: str, tipo_detectado: str) -> bool:
        """Avalia se a detec√ß√£o de tipo est√° correta"""
        # Padr√µes mais rigorosos para valida√ß√£o
        padroes_ea = [
            r'OnTick\s*\(',
            r'OrderSend\s*\(',
            r'trade\.Buy\s*\(',
            r'trade\.Sell\s*\(',
            r'PositionOpen\s*\(',
            r'#property\s+strict'
        ]
        
        padroes_indicator = [
            r'OnCalculate\s*\(',
            r'SetIndexBuffer\s*\(',
            r'IndicatorBuffers\s*\(',
            r'#property\s+indicator',
            r'PlotIndexSetInteger\s*\('
        ]
        
        padroes_script = [
            r'OnStart\s*\(',
            r'#property\s+script',
            r'void\s+OnStart\s*\('
        ]
        
        # Contagem de matches
        ea_matches = sum(1 for p in padroes_ea if re.search(p, codigo, re.IGNORECASE))
        ind_matches = sum(1 for p in padroes_indicator if re.search(p, codigo, re.IGNORECASE))
        scr_matches = sum(1 for p in padroes_script if re.search(p, codigo, re.IGNORECASE))
        
        # Determina tipo esperado
        if ea_matches >= 2:
            tipo_esperado = 'EA'
        elif ind_matches >= 2:
            tipo_esperado = 'Indicator'
        elif scr_matches >= 1:
            tipo_esperado = 'Script'
        else:
            tipo_esperado = 'Unknown'
        
        return tipo_detectado == tipo_esperado
    
    def avaliar_nomenclatura(self, nome_original: str, nome_sugerido: str) -> bool:
        """Avalia se a nomenclatura segue o padr√£o"""
        # Padr√£o: [PREFIXO]_[NOME]_v[VER]_[MERCADO].[EXT]
        padrao = r'^(EA|IND|SCR|STR|LIB)_[A-Za-z0-9]+_v\d+\.\d+_[A-Z0-9]+\.(mq[45]|pine)$'
        return bool(re.match(padrao, nome_sugerido))
    
    def avaliar_ftmo_compliance(self, analise_ftmo: Dict) -> bool:
        """Avalia se a an√°lise FTMO est√° correta"""
        score = analise_ftmo.get('score', 0)
        nivel = analise_ftmo.get('compliance_level', 'Non_Compliant')
        
        # Valida√ß√£o de consist√™ncia
        if score >= 6 and nivel != 'FTMO_Ready':
            return False
        if score >= 4 and nivel == 'Non_Compliant':
            return False
        if score < 2 and nivel == 'FTMO_Ready':
            return False
        
        return True
    
    def detectar_padroes_emergentes(self, metadata: Dict):
        """Detecta novos padr√µes que podem indicar novas categorias"""
        estrategia = metadata.get('classification', {}).get('strategy', 'Unknown')
        mercado = metadata.get('classification', {}).get('markets', ['Unknown'])[0]
        
        # Registra combina√ß√µes
        combinacao = f"{estrategia}_{mercado}"
        if combinacao not in self.padroes_emergentes:
            self.padroes_emergentes[combinacao] = 0
        self.padroes_emergentes[combinacao] += 1
    
    def registrar_processamento(self, metadata: Dict, tempo_processamento: float):
        """Registra m√©tricas de um arquivo processado"""
        import time
        start_time = time.time()
        
        self.metricas.total_arquivos += 1
        self.metricas.tempo_processamento.append(tempo_processamento)
        
        # Avalia detec√ß√£o de tipo
        codigo = metadata.get('file_info', {}).get('content_preview', '')
        tipo_detectado = metadata.get('classification', {}).get('file_type', 'Unknown')
        
        if self.avaliar_deteccao_tipo(codigo, tipo_detectado):
            self.metricas.deteccoes_corretas += 1
        else:
            self.metricas.deteccoes_incertas += 1
        
        # Avalia nomenclatura
        nome_original = metadata.get('file_info', {}).get('original_name', '')
        nome_sugerido = metadata.get('organization', {}).get('suggested_name', '')
        
        if self.avaliar_nomenclatura(nome_original, nome_sugerido):
            self.metricas.nomenclatura_consistente += 1
        
        # Avalia FTMO compliance
        analise_ftmo = metadata.get('ftmo_analysis', {})
        if self.avaliar_ftmo_compliance(analise_ftmo):
            self.metricas.ftmo_compliance_detectado += 1
        
        # Registra confidence score
        confidence = metadata.get('analysis_metadata', {}).get('confidence_score', 0)
        self.metricas.confidence_scores.append(confidence)
        
        # Detecta casos especiais
        if metadata.get('special_analysis', {}).get('is_exceptional', False):
            self.metricas.casos_especiais_detectados += 1
        
        # Detecta padr√µes emergentes
        self.detectar_padroes_emergentes(metadata)
        
        # Auto-avalia√ß√£o peri√≥dica
        if self.metricas.total_arquivos % self.intervalo_avaliacao == 0:
            self.executar_auto_avaliacao()
    
    def executar_auto_avaliacao(self) -> Dict:
        """Executa auto-avalia√ß√£o completa"""
        print(f"\nüîç AUTO-AVALIA√á√ÉO - Arquivo {self.metricas.total_arquivos}")
        print("=" * 50)
        
        avaliacao = {
            'timestamp': datetime.now().isoformat(),
            'arquivos_processados': self.metricas.total_arquivos,
            'metricas': {
                'taxa_precisao': self.metricas.taxa_precisao,
                'confidence_media': self.metricas.confidence_media,
                'tempo_medio': self.metricas.tempo_medio,
                'casos_especiais': self.metricas.casos_especiais_detectados
            },
            'status_qualidade': self.avaliar_qualidade_geral(),
            'ajustes_sugeridos': self.sugerir_ajustes(),
            'padroes_emergentes': self.identificar_padroes_emergentes()
        }
        
        self.historico_avaliacoes.append(avaliacao)
        self.imprimir_relatorio_avaliacao(avaliacao)
        
        return avaliacao
    
    def avaliar_qualidade_geral(self) -> str:
        """Avalia qualidade geral do processo"""
        score = 0
        
        # Taxa de precis√£o
        if self.metricas.taxa_precisao >= self.thresholds['taxa_precisao_minima']:
            score += 25
        elif self.metricas.taxa_precisao >= 70:
            score += 15
        
        # Confidence m√©dio
        if self.metricas.confidence_media >= self.thresholds['confidence_minimo']:
            score += 25
        elif self.metricas.confidence_media >= 50:
            score += 15
        
        # Tempo de processamento
        if self.metricas.tempo_medio <= self.thresholds['tempo_maximo_por_arquivo']:
            score += 25
        elif self.metricas.tempo_medio <= 10:
            score += 15
        
        # Detec√ß√£o de casos especiais
        taxa_especiais = (self.metricas.casos_especiais_detectados / self.metricas.total_arquivos) * 100
        if taxa_especiais >= self.thresholds['casos_especiais_minimos']:
            score += 25
        elif taxa_especiais >= 2:
            score += 15
        
        if score >= 90:
            return "EXCELENTE"
        elif score >= 70:
            return "BOM"
        elif score >= 50:
            return "REGULAR"
        else:
            return "PRECISA_MELHORAR"
    
    def sugerir_ajustes(self) -> List[str]:
        """Sugere ajustes baseados nas m√©tricas"""
        ajustes = []
        
        if self.metricas.taxa_precisao < self.thresholds['taxa_precisao_minima']:
            ajustes.append("üîß Melhorar padr√µes de detec√ß√£o de tipo")
            ajustes.append("üìö Adicionar mais keywords espec√≠ficas")
        
        if self.metricas.confidence_media < self.thresholds['confidence_minimo']:
            ajustes.append("‚ö° Refinar algoritmo de confidence scoring")
            ajustes.append("üéØ Adicionar valida√ß√£o cruzada de padr√µes")
        
        if self.metricas.tempo_medio > self.thresholds['tempo_maximo_por_arquivo']:
            ajustes.append("üöÄ Otimizar performance de an√°lise")
            ajustes.append("üíæ Implementar cache de padr√µes")
        
        taxa_especiais = (self.metricas.casos_especiais_detectados / self.metricas.total_arquivos) * 100
        if taxa_especiais < self.thresholds['casos_especiais_minimos']:
            ajustes.append("üîç Melhorar detec√ß√£o de casos especiais")
            ajustes.append("‚≠ê Adicionar mais padr√µes de qualidade")
        
        return ajustes
    
    def identificar_padroes_emergentes(self) -> List[str]:
        """Identifica padr√µes emergentes que podem virar novas categorias"""
        emergentes = []
        
        for padrao, count in self.padroes_emergentes.items():
            if count >= 5:  # Threshold para nova categoria
                emergentes.append(f"üìà Padr√£o '{padrao}': {count} ocorr√™ncias - Considerar nova categoria")
        
        return emergentes
    
    def imprimir_relatorio_avaliacao(self, avaliacao: Dict):
        """Imprime relat√≥rio de auto-avalia√ß√£o"""
        print(f"üìä Taxa de Precis√£o: {avaliacao['metricas']['taxa_precisao']:.1f}%")
        print(f"üéØ Confidence M√©dio: {avaliacao['metricas']['confidence_media']:.1f}%")
        print(f"‚è±Ô∏è Tempo M√©dio: {avaliacao['metricas']['tempo_medio']:.2f}s/arquivo")
        print(f"‚≠ê Casos Especiais: {avaliacao['metricas']['casos_especiais']}")
        print(f"üèÜ Status Geral: {avaliacao['status_qualidade']}")
        
        if avaliacao['ajustes_sugeridos']:
            print("\nüîß AJUSTES SUGERIDOS:")
            for ajuste in avaliacao['ajustes_sugeridos']:
                print(f"   {ajuste}")
        
        if avaliacao['padroes_emergentes']:
            print("\nüìà PADR√ïES EMERGENTES:")
            for padrao in avaliacao['padroes_emergentes']:
                print(f"   {padrao}")
        
        print("=" * 50)
    
    def gerar_relatorio_final(self) -> Dict:
        """Gera relat√≥rio final de auto-avalia√ß√£o"""
        return {
            'resumo_geral': {
                'total_arquivos': self.metricas.total_arquivos,
                'taxa_precisao_final': self.metricas.taxa_precisao,
                'confidence_media_final': self.metricas.confidence_media,
                'tempo_total': sum(self.metricas.tempo_processamento),
                'casos_especiais_total': self.metricas.casos_especiais_detectados
            },
            'evolucao_qualidade': self.historico_avaliacoes,
            'padroes_emergentes_finais': self.padroes_emergentes,
            'ajustes_realizados': self.ajustes_realizados,
            'recomendacoes_futuras': self.gerar_recomendacoes_futuras()
        }
    
    def gerar_recomendacoes_futuras(self) -> List[str]:
        """Gera recomenda√ß√µes para melhorias futuras"""
        recomendacoes = []
        
        # An√°lise de tend√™ncias
        if len(self.historico_avaliacoes) >= 3:
            ultimas_precisoes = [a['metricas']['taxa_precisao'] for a in self.historico_avaliacoes[-3:]]
            if ultimas_precisoes[-1] < ultimas_precisoes[0]:
                recomendacoes.append("üìâ Precis√£o em decl√≠nio - Revisar padr√µes de detec√ß√£o")
        
        # Padr√µes emergentes
        for padrao, count in self.padroes_emergentes.items():
            if count >= 10:
                recomendacoes.append(f"üÜï Criar categoria espec√≠fica para '{padrao}' ({count} arquivos)")
        
        # Performance
        if self.metricas.tempo_medio > 3:
            recomendacoes.append("‚ö° Implementar otimiza√ß√µes de performance")
        
        return recomendacoes

# Exemplo de uso integrado
if __name__ == "__main__":
    # Demonstra√ß√£o do sistema de auto-avalia√ß√£o
    avaliador = AutoAvaliadorQualidade(intervalo_avaliacao=5)
    
    print("üéØ SISTEMA DE AUTO-AVALIA√á√ÉO ATIVADO")
    print("Monitoramento cont√≠nuo da qualidade do processo...\n")
    
    # Simula√ß√£o de processamento com auto-avalia√ß√£o
    arquivos_teste = [
        "EA_Test1.mq4", "IND_Test2.mq4", "SCR_Test3.mq4",
        "EA_Test4.mq5", "IND_Test5.mq5", "Unknown_Test6.mq4"
    ]
    
    for i, arquivo in enumerate(arquivos_teste):
        # Simula metadata de processamento
        metadata_simulado = {
            'file_info': {
                'original_name': arquivo,
                'content_preview': 'OnTick() { OrderSend(); }' if 'EA_' in arquivo else 'OnCalculate() { SetIndexBuffer(); }'
            },
            'classification': {
                'file_type': 'EA' if 'EA_' in arquivo else 'Indicator' if 'IND_' in arquivo else 'Script',
                'strategy': 'scalping',
                'markets': ['EURUSD']
            },
            'organization': {
                'suggested_name': arquivo.replace('.mq', '_v1.0_EURUSD.mq')
            },
            'ftmo_analysis': {
                'score': 5,
                'compliance_level': 'Partially_Compliant'
            },
            'analysis_metadata': {
                'confidence_score': 85.0
            },
            'special_analysis': {
                'is_exceptional': i % 3 == 0  # Simula casos especiais
            }
        }
        
        # Registra processamento
        avaliador.registrar_processamento(metadata_simulado, 2.5)
        
        print(f"‚úÖ Processado: {arquivo}")
    
    # Relat√≥rio final
    print("\nüìã RELAT√ìRIO FINAL DE AUTO-AVALIA√á√ÉO")
    relatorio_final = avaliador.gerar_relatorio_final()
    print(json.dumps(relatorio_final, indent=2, ensure_ascii=False))